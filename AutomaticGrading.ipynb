{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re \n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = open(\"/Users/savita/Desktop/computeCost.txt\",\"r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = open(\"/Users/savita/Desktop/gradientDescent.txt\",\"r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents1 = text1.read()\n",
    "file_contents2 = text2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function J = computeCost(X, y, theta)\\n%COMPUTECOST Compute cost for linear regression\\n%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\\n%   parameter for linear regression to fit the data points in X and y\\n\\n% Initialize some useful values\\nm = length(X); % number of training examples\\n% ====================== YOUR CODE HERE ======================\\n% Instructions: Compute the cost of a particular choice of theta\\n%               You should set J to the cost.\\nJ = 0;\\nJ=1/(2*m)*(sum(((X*theta)-y).^2));\\n% =========================================================================\\nend\\n\\n\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\\n%GRADIENTDESCENT Performs gradient descent to learn theta\\n%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \\n%   taking num_iters gradient steps with learning rate alpha\\n\\n% Initialize some useful values\\nm = length(X); % number of training examples\\nJ_history = zeros(num_iters, 1);\\n\\nfor iter = 1:num_iters\\n    % ====================== YOUR CODE HERE ======================\\n    % Instructions: Perform a single gradient step on the parameter vector\\n    %               theta. \\n    %\\n    % Hint: While debugging, it can be useful to print out the values\\n    %       of the cost function (computeCost) and gradient here.\\n    dell=(1/m)*((X*theta-y)' * X)';\\n    theta = theta - (alpha * dell);  \\n    % ============================================================\\n    % Save the cost J in every iteration    \\n    J_history(iter) = computeCost(X, y, theta);\\nend\\n\\nend\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/savita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text1 = re.sub(r'%(.*)\\n','',file_contents1)\n",
    "clean_text1 = re.sub(r'\\s+',' ',clean_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function J = computeCost(X, y, theta) m = length(X); J = 0; J=1/(2*m)*(sum(((X*theta)-y).^2)); end '"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters) m = length(X); J_history = zeros(num_iters, 1); for iter = 1:num_iters dell=(1/m)*((X*theta-y)' * X)'; theta = theta - (alpha * dell); J_history(iter) = computeCost(X, y, theta); end end\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text2 = re.sub(r'%(.*)\\n','',file_contents2)\n",
    "clean_text2 = re.sub(r'\\s+',' ',clean_text2)\n",
    "clean_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "text = [[' '.join(nltk.word_tokenize(document)[i:i+n]) for i in range(len(nltk.word_tokenize(document))-n)]\n",
    "         for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['function J =', 'J = computeCost', '= computeCost (', 'computeCost ( X', '( X ,', 'X , y', ', y ,', 'y , theta', ', theta )', 'theta ) m', ') m =', 'm = length', '= length (', 'length ( X', '( X )', 'X ) ;', ') ; J', '; J =', 'J = 0', '= 0 ;', '0 ; J=1/', '; J=1/ (', 'J=1/ ( 2*m', '( 2*m )', '2*m ) *', ') * (', '* ( sum', '( sum (', 'sum ( (', '( ( (', '( ( X*theta', '( X*theta )', 'X*theta ) -y', ') -y )', '-y ) .^2', ') .^2 )', '.^2 ) )', ') ) ;'], ['function [ theta', '[ theta ,', 'theta , J_history', ', J_history ]', 'J_history ] =', '] = gradientDescent', '= gradientDescent (', 'gradientDescent ( X', '( X ,', 'X , y', ', y ,', 'y , theta', ', theta ,', 'theta , alpha', ', alpha ,', 'alpha , num_iters', ', num_iters )', 'num_iters ) m', ') m =', 'm = length', '= length (', 'length ( X', '( X )', 'X ) ;', ') ; J_history', '; J_history =', 'J_history = zeros', '= zeros (', 'zeros ( num_iters', '( num_iters ,', 'num_iters , 1', ', 1 )', '1 ) ;', ') ; for', '; for iter', 'for iter =', 'iter = 1', '= 1 :', '1 : num_iters', ': num_iters dell=', 'num_iters dell= (', 'dell= ( 1/m', '( 1/m )', '1/m ) *', ') * (', '* ( (', '( ( X*theta-y', '( X*theta-y )', \"X*theta-y ) '\", \") ' *\", \"' * X\", '* X )', \"X ) '\", \") ' ;\", \"' ; theta\", '; theta =', 'theta = theta', '= theta -', 'theta - (', '- ( alpha', '( alpha *', 'alpha * dell', '* dell )', 'dell ) ;', ') ; J_history', '; J_history (', 'J_history ( iter', '( iter )', 'iter ) =', ') = computeCost', '= computeCost (', 'computeCost ( X', '( X ,', 'X , y', ', y ,', 'y , theta', ', theta )', 'theta ) ;', ') ; end']]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "for i in range(len(words)-n):\n",
    "    gram =' '.join(words[i:i+n])\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(words[i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =[clean_text1,clean_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function J = computeCost(X, y, theta) m = length(X); J = 0; J=1/(2*m)*(sum(((X*theta)-y).^2)); end ',\n",
       " \"function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters) m = length(X); J_history = zeros(num_iters, 1); for iter = 1:num_iters dell=(1/m)*((X*theta-y)' * X)'; theta = theta - (alpha * dell); J_history(iter) = computeCost(X, y, theta); end end\"]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [[word for word in nltk.word_tokenize(document)]\n",
    "         for document in documents]\n",
    "\n",
    "#adding the trigram fetures to the corpus to increase the semantic similarity \n",
    "n=3\n",
    "text2 = [[' '.join(nltk.word_tokenize(document)[i:i+n]) for i in range(len(nltk.word_tokenize(document))-n)]\n",
    "         for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text1 + text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = clean_text2\n",
    "vec_bow = dictionary.doc2bow(nltk.word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.99999994), (0, 0.83837235), (2, 0.0), (3, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
